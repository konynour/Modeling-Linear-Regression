# Deep Learning for House Price Prediction üè†


Welcome to my **Deep Learning for House Price Prediction** project! This repository contains a neural network model built with TensorFlow and Keras to predict house prices based on the Boston Housing dataset. The goal was to explore deep learning concepts like neural network architecture, regularization, and hyperparameter tuning while solving a real-world regression problem.

## üìñ Project Overview

In this project, I developed a feedforward neural network to predict the median house prices in Boston using features like the average number of rooms. I experimented with different architectures, optimizers, and regularization techniques to optimize the model‚Äôs performance. The project demonstrates my understanding of:
- Building and training neural networks with TensorFlow/Keras.
- Evaluating model performance using loss curves and validation metrics.
- Visualizing predictions and data with Matplotlib.
- Handling regression tasks in machine learning.

## üéØ Objectives

- **Predict House Prices**: Use the Boston Housing dataset to predict house prices based on a single feature (average number of rooms).
- **Experiment with Architectures**: Test different neural network configurations (e.g., number of layers and neurons) to find the best model.
- **Optimize Performance**: Apply techniques like L2 regularization and learning rate tuning to reduce overfitting and improve generalization.
- **Visualize Results**: Plot training/validation loss and predictions to understand model behavior.

## üõ†Ô∏è Methodology

### Dataset
- **Boston Housing Dataset**: Contains 506 samples with 13 features (I focused on the average number of rooms for simplicity).
- **Preprocessing**: Split data into training (70%) and validation (30%) sets. (Note: I plan to add full feature usage and normalization in future updates.)

### Models
I experimented with three neural network architectures:
1. **Model 1**: 2 hidden layers (20 and 10 neurons, ReLU), RMSprop optimizer (learning rate = 0.005), no regularization.
   - Training Loss: 41.83, Validation Loss: 61.97
2. **Model 2**: Same as Model 1 but with L2 regularization (0.01) and learning rate = 0.001.
   - Training Loss: 47.78, Validation Loss: 72.89
3. **Model 3**: 2 hidden layers (10 and 5 neurons, ReLU), RMSprop (learning rate = 0.005), no regularization.
   - Training Loss: 46.14, Validation Loss: 69.26

### Key Findings
- **Model 1** performed best with the lowest validation loss (61.97), indicating better generalization.
- L2 regularization (Model 2) was too strong, increasing both training and validation loss.
- Reducing complexity (Model 3) slightly improved overfitting but at the cost of higher loss compared to Model 1.
- Loss curves showed quick convergence within 20 epochs, but overfitting remained a challenge due to using a single feature.

### Visualizations
Below are the loss curves for Model 1 (best performing):

![Loss Curve](https://github.com/konynour/Modeling-Linear-Regression/blob/main/Screenshot_20.png)

And a plot of predictions vs. ground truth:

![Predictions Plot](path/to/predictions_plot.png)

## üöÄ How to Run the Project

### Prerequisites
- Python 3.8+
- Libraries: `tensorflow`, `numpy`, `matplotlib`, `scikit-learn`

Install dependencies:
```bash
pip install tensorflow numpy matplotlib scikit-learn
```

### Steps
1. **Clone the Repository**:
   ```bash
   git clone https://github.com/konynour/Modeling-Linear-Regression.git
   cd MODELING LINEAR REGRESSION
   ```

2. **Run the Code**:
   - Open `0x.ipynb` in Jupyter Notebook or run the Python script:
     ```bash
     python 0x.py
     ```
   - The script will train the model, display loss curves, and generate predictions.

3. **Explore Results**:
   - Check the `plots/` folder for loss curves and prediction visualizations.
   - Run `predict.py` to make custom predictions:
     ```python
     import numpy as np
     x = np.array([3, 4, 5, 6, 7])
     y_pred = model.predict(x)
     for idx, pred in enumerate(y_pred):
         print(f"Predicted price for {x[idx]} rooms: ${pred.item():.2f}K")
     ```

## üìä Results

- **Best Model (Model 1)**:
  - Training Loss: 41.83
  - Validation Loss: 61.97
  - Predictions Example:
    - 3 rooms: ~$11.7K
    - 5 rooms: ~$18.1K
    - 7 rooms: ~$24.5K
- **Challenges**:
  - Overfitting due to using a single feature.
  - Limited data complexity with one input feature.
- **Future Improvements**:
  - Use all 13 features from the dataset.
  - Apply data normalization for better convergence.
  - Experiment with dropout or batch normalization.

